- Implement and test cyclic LR scheduling

- Implement and test wCE + Dice loss

- Implement option to save model output as foreground probabilities in inference



- Implement validation metrics:
    x Dice and Jaccard for now. 
    - Then Surface Dice


- Improve WandB logging:
    - Log system-related stuff 
    - Log richer info - prediction, etc.


- Efficiency improvement:
    - torch.cuda.empty_cache() -- read more about this
    - Option for distributed training: See torch DataParallel for now. Later move on to using torch DistributedDataParallel 

- Integrate enable_distributed option of the Trainer and Infer into the config system